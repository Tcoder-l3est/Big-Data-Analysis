# BIG DATA课程笔记

## 导论

大数据的四大特征

​	**大量化、快速化、多样化、价值化**

大数据分析的三个特征

​	**全样而非抽样、效率而非精确、相关而非因果**

大数据处理模型：

​	批处理：以“静止数据”为出发点，处理逻辑进来，算完后价值出去

​	流数据：不动的是逻辑，“动态数据”进来，计算完后价值留下，原始数据加入“静止数		据”，或索性丢弃

其中批处理包括MapReduce模型

大数据技术和工具

- NoSQL，not only sql，模式自由、简易备份、简单API、不支持ACID但是支持BASE、支持海量数据
  - 做了查询优化
    - 利用MapReduce时并行查询、每个MapTask处理一部分
  - 采用索引技术优化多指查询，多维索引
- 流处理模式：Storm
- 工具：Hadoop、Spark

---

## Hadoop 简介

### 概念

1. MapReduce采用了分布式文件系统即HDFS，也就是Hadoop 分布式文件系统，MapReduce是用来整合DFS的数据的

2. 存储冗余数据保证安全性

3. 对比关系型数据库RDBMS，RDBMS适合查询更新、MapReduce适合批处理。RDBMS 适合持续更新的数据集，MapReduce适合一次写入多次读取。RDBNMS只能处理结构化数据，MapReduce能够处理半结构化或者非结构化。

   ![image-20220405200729138](https://vvtorres.oss-cn-beijing.aliyuncs.com/image-20220405200729138.png)

### 体系结构



![image-20220405201056878](https://vvtorres.oss-cn-beijing.aliyuncs.com/image-20220405201056878.png)

- 最底部是Hadoop分布式文件系统（HDFS），它存储Hadoop集群中所有存储节点上的文件

- HDFS的上一层是MapReduce引擎
- HBase位于结构化存储层，是一个分布式的列存储数据库
- Zookeeper是一个分布式的、高可用性的协调服务，提供分布式锁之类的基本服务
- Hive是一个建立在Hadoop 基础之上的数据仓库，用于管理存储于HDFS或Hbase中的结构化/半结构化数据
- Pig提供一种数据流语言，程序员可以将复杂的数据分析任务实现为Pig操作上的数据流脚本，这些脚本可自动转换为MapReduce任务链，在Hadoop上执行，从而简化程序员的数据分析工作难度
- Sqoop是SQL-to-Hadoop的缩写，为在RDBMS与Hadoop平台（HDFS, Hbase, Hive）间进行快速批量数据交换
- Avro是个数据序列化的系统，用于将数据对象转换成便于数据存储和网络传输的格式
- 一个HDFS集群是由一个NameNode和若干个DataNode组成
- NameNode作为主服务器，**管理文件系统的命名空间和客户端对文件的访问操作**；集群中的**DataNode管理存储的数据**
- HDFS支持用户以文件的形式存储数据，文件被分成若干个数据块，而且这若干个数据块存放在一组DataNode上
- 具体到程序层面：MapReduce框架是由一个单独运行在主节点上的JobTracker 和运行在每个集群从节点上的TaskTracker共同组成的
  - 主节点负责调度构成一个作业的所有任务，这些任务分布在不同的从节点上；主节点监控它们的执行情况，并且重新执行之前失败的任务。从节点仅负责由主节点指派的任务
  - 当一个Job 被提交时，JobTracker接收到提交作业和配置信息之后，就会将配置信息等分发给从节点，同时调度任务并监控TaskTracker的执行
- TaskTracker和DataNode需配对的设置在同一个物理的从节点服务器上；JobTracker和NameNode可以设置在同一个物理主控节点服务器上，也可以分开设置

### Hadoop与分布式开发

![image-20220405202959992](https://vvtorres.oss-cn-beijing.aliyuncs.com/image-20220405202959992.png)

![image-20220405204705230](https://vvtorres.oss-cn-beijing.aliyuncs.com/image-20220405204705230.png)

**编程原理**

- 利用一个输入的key/value 对集合,来产生一个输出的key/value 对集合。这个过程基于Map 和Reduce这两个用户自定义函数实现
- 用户自定义的map函数接收一个输入的key/value 对，然后产生一个中间key/value 对的集合
- MapReduce（shuffle）把所有具有相同key值的value集合在一起，然后传递给reduce 函数
- 用户自定义的reduce 函数接收key和相关的value集合，合并这些value 值，形成一个较小的value 集合。一般来说，每次reduce 函数调用只产生0 或1 个输出的value值
- 通常通过一个迭代器把中间的value 值提供给reduce 函数，这样就可以处理无法全部放入内存中的大量的value 值集合

**基于Map和Reduce的并行计算模型**

![image-20220405203254179](https://vvtorres.oss-cn-beijing.aliyuncs.com/image-20220405203254179.png)

其中

- Barrier是同步障

---

## 存储——HDFS

### 概念

- HDFS的块是抽象的概念，默认块为64MB，比一般文件系统中所说的块（若干KB）要大得多
  - 好处：文件大、便于操作、复制容错
- 两类节点：NameNode和DataNode
  - NameNode：集群里面的主节点，负责管理整个HDFS系统的命名空间和元数据，也是客户端访问HDFS系统的入口
  - 三种**元数据metadata如下**
    - 命名空间：即目录结构、命名空间维护操作包括文件和目录的创建删除重命名
    - 数据块和文件名的映射表：客户点需要访问NameNode才知道一个文件所有的数据块保存在那些DataNode上
    - 数据块副本的位置信息，默认三个副本，可以指定部分银子Replication Factor
    - 承担Master任务
  - DataNode：负责数据读取和存储
    - 写入时：目录节点分配数据块，直接写入对应的DataNode
    - 读取：客户点从NameNode获得映射关系后，直接到DataNode读
    - DataNode根据NameNode的命令创建、删除数据块，副本复制

### HDFS 体系结构

![image-20220405205441702](https://vvtorres.oss-cn-beijing.aliyuncs.com/image-20220405205441702.png)

- HDFS是经典的主从结构
- HDFS的基本文件的访问过程
  - 通过HDFS客户端，奖文件名发送到NameNode
  - 接收后，在HDFS目录中检索文件名对应的数据库，找到对应的DataNode，然后送回客户端
  - 接收后，与DataNode并行进行数据传输，提交操作日志到NameNode
- HDFS不允许使用链接
- 为了保证可靠性，底层使用TCP来传输。
  - 应用向NameNode主动发出TCP链接，交互的协议成为Client协议
  - NameNode与DataNode传输的协议成为DataNode协议
  - 用户与Datanode的协议是通过远程过程调用RPC、并由NameNode相应来实现
  - 如下图

![image-20220405213033545](https://vvtorres.oss-cn-beijing.aliyuncs.com/image-20220405213033545.png)

![image-20220405210648449](https://vvtorres.oss-cn-beijing.aliyuncs.com/image-20220405210648449.png)

这个图可以看出什么信息呢？

- Client会从Datanodes读
- Client写的时候 会写副本吗？
- Rack是机柜，Replication是复制、
- Namenode会对Datanodes进行快操作

### HDFS可靠性设计

- 除了文件的最后一块以外其它所有的数据块都是固定大小的，为了数据容错性，每一个数据块都会被冗余存储起来

- 目录节点根据数据块的副本状况来作出处理决策，数据节点会定期发送一个**心跳信号（Heartbeat）**和数据块列表给目录节点，心跳信号使目录节点知道该数据节点还是有效的，而数据块列表包括了该数据节点上面的所有数据块编号

- 数据节点出错的处理

  - 每个数据节点会定时发送一个心跳信息给目录节点，表明自己仍然存活

  - 网络异常可能会导致一部分数据节点无法和目录节点通讯，这时候目录节点收不到心跳信息，就认为该数据节点已死机

    然后：从有效数据节点列表中清除它，而该数据节点上面的所有数据块也会被标记为不可读。目录节点会定期检查副本数，进行复制。

- 数据异常

  - 客户端需实现对数据块的校验，保证数据一致性
  - 在创建文件的时候，HDFS会为文件生成一个校验和（CheckSum），校验和文件与文件本身保存在同一个空间中。数据传输时，将数据与校验和一起传输，客户端对每个读取的数据块进行校验
  - 如果校验出错，客户端就会请求到另外一个数据节点读取该文件块，并且报告给目录节点这个文件块有错误，目录节点会重新复制这个块

- 目录节点出错

  - HDFS刚启动时，目录节点进入安全模式（safe mode），**此时不做任何文件修改操作**；目录节点和各个数据节点通信，获得数据块信息，并对数据块信息进行检查；认为安全的块比例超过阈值，才退出安全模式

  - 辅助目录节点，用来备份目录节点的元数据，当目录节点失效时，从辅助目录节点恢复出目录节点的元数据

  - 文件镜像数据**FsImage**和编辑日志数据**Editlog**是目录节点元数据中最重要的部分，前者相当于HDFS的**检查点**，后者记录对HDFS的**最新修改信息**

  - 恢复：目录节点启动时，读取FsImage的内容到内存，并将其与Editlog中的所有修改信息合并生成最新的FsImage；目录节点运行中，所有关于HDFS的修改信息，都将写入Editlog

    - FsImage和Editlog时最核心的数据结构
    - 所以存在多个备份文件，在辅助目录节点上再进行备份，增加负担提高可靠性。维护同步

    

### HDFS存储原理

- 以机柜为基础
- 副本因子为3：HDFS将两份放在同一个rack id中，另一个放在不同的rankid的机器上
- 实战中Namenode
  - ![image-20220405213805409](https://vvtorres.oss-cn-beijing.aliyuncs.com/image-20220405213805409.png)
  - ![image-20220405213854377](https://vvtorres.oss-cn-beijing.aliyuncs.com/image-20220405213854377.png)
  - ![image-20220405213941210](https://vvtorres.oss-cn-beijing.aliyuncs.com/image-20220405213941210.png)
- DataNode
  - ![image-20220405214008805](https://vvtorres.oss-cn-beijing.aliyuncs.com/image-20220405214008805.png)
  - ![image-20220405214230211](https://vvtorres.oss-cn-beijing.aliyuncs.com/image-20220405214230211.png)
  - 

- **数据读取时**
  - 如果有块数据和客户端的机柜id一样，就优先选择该数据节点，客户端直接和数据节点建立连接，读取数据
  - 如果没有，可以随机选取一个数据节点

![image-20220405212824697](https://vvtorres.oss-cn-beijing.aliyuncs.com/image-20220405212824697.png)

![image-20220405212932527](https://vvtorres.oss-cn-beijing.aliyuncs.com/image-20220405212932527.png)

- **数据复制**
  - **流水线复制策略**
  - 当在HDFS上写文件：
    - 先写在本地
    - 对文件分块，64MB
    - 每块数据对HDFS目录节点发起写入请求
    - NameNode选一个DataNode列表，返回给客户端
    - 客户端把数据写入第一台DataNode，列表传给DataNode
    - 当数据节点接收到4KB数据时，写入本地，并且发起到下一台Datanode的连接，传4K，流水线
    - 当文件写完时，数据复制也同时完成？
- **数据写入**
  - ![image-20220405214436163](https://vvtorres.oss-cn-beijing.aliyuncs.com/image-20220405214436163.png)
  - ![image-20220405214544884](https://vvtorres.oss-cn-beijing.aliyuncs.com/image-20220405214544884.png)

---

### HDFS文件系统操作命令



#### 启动

```shell
start-dfs.sh
# 会启动namenode，然后根据conf/slaves记录诸葛启动Datanode
# 根据 conf/masters中记录的SecondaryNameNode地址 启动SNameNode
stop-dfs.sh
```

#### 文件操作

```shell
# 数值指定的文件到stdout
hadoop dfs-cat URI [URI..] 
```

- chgrp : hadoop dfs -chgrp [-R] GROUP URI [URI ...]
  - 改变文件所属的用户组，-R对目录结构起作用
- chmod: hadoop dfs -chmod [-R] 
  - 改变文件权限
- chown 改变文件所属用户
- copyFromLocal
  - hadoop dfs -copyFromLocal \<localsrc\> URI
  - put命令
- copyToLocal
  - hadoop dfs -copyToLocal \<-ignorecrc\> [-crc] URI \<localdst\>
  - get 命令
- count
  - hadoop dfs -count [-q] \<paths\>
  - 统计匹配对应路径下的目录数、文件数、字节数
- cp
  - 对dfs的文件进行互相copy
- du
  - dfs -du [-s] [-h] URI [URI ...]
  - 显示目录下所有目录+文件大小；
  - 显示文件大小
  - -s是累加和
  - -h 是16进制？
- dus
- expunge 清空回收站
  - hadoop dfs -expunge
- get
  - 将文件拷贝到本地
  - hadoop dfs -get [-ignorecrc] [-crc] \<src> \<localdst>
  - CRC校验失败可以通过-ignorecrc 拷贝
- getmerge
  - hadoop dfs -getmerge \<src>\<localdst>  [addnl]
  - 将源文件目录下所有文件排序后合并到目的文件中
  - 添加addnl 可以在每个文件后面插入新行
- ls
  - 列出文件权限、副本个数、用户ID、组ID、文件大小、最近一次修改日期、时间、文件名
- lsr
- mkdir
- moveFromLocal
  - 源文件上传之后被删除
- moveToLocal
  - 下载后删除
- mv 删除
- put
  - 上传
- rm 删除参数指定的文件，只能删除文件和非空目录
  - hadoop dfs -rm [-skipTrash] URI [URI..]
  - skipTrash 不放到回收站直接删除
- rmr
- setrep 改变副本格式
- stat 路径状态
- tail
- test
  - hadoop dfs -test -[ezd] URI
  - -e 检查是否存在 存在返回0
  - -z 检查大小是否为0 是返回0
  - -d 检查路径是否为目录 是返回0
- text 将文本文件或某些格式的非文本文件输出
- touchz 创建一个大小为0的文件



### HDFS 基本接口

- 基本所有的文件API来自于FileSystem类

---

## MapReduce 并行编程框架

### 并行计算

划分子任务、计算、合并结果

![image-20220406145858596](https://vvtorres.oss-cn-beijing.aliyuncs.com/image-20220406145858596.png)

### 构抽象模型Map与Reduce

#### Map

对一组数据元素进行某种重复式的处理

**map**: **(k1; v1)**$\rightarrow$ **[(k2; v2)]**

输入：键值对(k1; v1)表示的数据

处理：文档数据记录(如文本文件中的行，或数据表格中的行)将以“键值对”形式传入map函数；map函数将处理这些键值对，并以另一种键值对形式输出处理的一组键值对中间结果[(k2; v2)]

输出：键值对[(k2; v2)]表示的一组中间数据



#### Reduce

对Map的中间结果进行某种进一步的结果整理

例如关系数据库的聚合函数

**reduce**: **(k2; [v2])** $\rightarrow$  **[(k3; v3)]**

输入： 由map输出的一组键值对[(k2; v2)] 将被进行合并处理将同样主键下的不同数值合并到一个列表[v2]中，故reduce的输入为(k2; [v2]) 

处理：对传入的中间结果列表数据进行某种整理或进一步的处理,并产生最终的某种形式的结果输出[(k3; v3)] 。

输出：最终输出结果[(k3; v3)] 

---

![image-20220406150612183](https://vvtorres.oss-cn-beijing.aliyuncs.com/image-20220406150612183.png)

Reduce前，必须所有map都做完，因此需要一个同步障barrier

这个阶段也负责对map中间结果数据进行收集整理和处理(aggregation、shuffle)

### 编程模型和框架

1. wordcount 伪代码

![image-20220406151147733](https://vvtorres.oss-cn-beijing.aliyuncs.com/image-20220406151147733.png)

2. Combiner和Partitioner

   - Combiner

     在每个map节点输出中间结果的键值对前，进行合并处理，

     例如两个(good,1) 合并成(good,2)

     执行是在Map节点完成计算、输出中间结果之前

   - Partitioner

     分区处理

     目的：消除数据传入Reduce结点之后带来的不必要的相关性。

     在Map到Reduce中间数据整理阶段完成

     例如：保证所有主键相同的\<k,v>都输入到同一个Reduce节点，避免Reduce再访问其他Reduce

3. 完整的MapReduce并行编程框架

![image-20220406152908320](https://vvtorres.oss-cn-beijing.aliyuncs.com/image-20220406152908320.png)

4. MapReduce提供了统一的计算框架可以完成
   - 计算任务的划分和调度
   - 数据分布存储和划分
   - 处理数据和计算任务同步
   - 结果数据的收集整理：sorting、combining、partitioning
   - 系统通信、负载平衡、计算性能优化处理
   - 节点出错检测、失效恢复
5. MapReduce提供的主要功能
   1. 任务调度：提交的一个计算作业(job)将被划分为很多个计算任务(tasks), 任务调度功能主要负责为这些划分后的计算任务分配和调度计算节点(map节点或reducer节点); 同时负责监控这些节点的执行状态, 并负责map节点执行的同步控制(barrier); 也负责进行一些计算性能优化处理, 如对最慢的计算任务采用多备份执行、选最快完成者作为结果
   2. 数据/代码互定位：为了减少数据通信，一个基本原则是本地化数据处理(locality)，即一个计算节点尽可能处理其本地磁盘上所分布存储的数据，这实现了代码向数据的迁移；当无法进行这种本地化数据处理时，再寻找其它可用节点并将数据从网络上传送给该节点(数据向代码迁移)，但将尽可能从数据所在的本地机架上寻找可用节点以减少通信延迟
   3. 出错处理：以低端商用服务器构成的大规模MapReduce计算集群中,节点硬件(主机、磁盘、内存等)出错和软件有bug是常态，因此,MapReducer需要能检测并隔离出错节点，并调度分配新的节点接管出错节点的计算任务
   4. 分布式数据存储与文件管理：海量数据处理需要一个良好的分布数据存储和文件管理系统支撑,该文件系统能够把海量数据分布存储在各个节点的本地磁盘上,但保持整个数据在逻辑上成为一个完整的数据文件；为了提供数据存储容错机制,该文件系统还要提供数据块的多备份存储管理能力
6. MapReduce的主要设计思想与特点
   - 向外扩展 而非向上纵向扩展
     - 即MapReduce集群的构筑选用价格便宜、易于扩展的大量低端商用服务器，而非价格昂贵、不易扩展的高端服务器（SMP）
   -  失效被认为是常态
     - 一个良好设计、具有容错性的并行计算系统不能因为节点失效而影响计算服务的质量，任何节点失效都不应当导致结果的不一致或不确定性；任何一个节点失效时，其它节点要能够无缝接管失效节点的计算任务；当失效节点恢复后应能自动无缝加入集群，而不需要管理员人工进行系统配置、
   - **把处理向数据迁移**
     - 计算节点将首先将尽量负责计算其本地存储的数据,以发挥数据本地化特点(locality),仅当节点无法处理本地数据时，再采用就近原则寻找其它可用计算节点，并把数据传送到该可用计算节点
   - **顺序处理数据、避免随机访问数据**
   -  **为应用开发者隐藏系统层细节**
   - **平滑无缝的可扩展性**

### 基本工作原理





### 主要组件以及编程接口

#### 输入格式InputFormat

- InputFormat提供了以下一些功能
  - 选择文件或者其它对象，用来作为输入
  - 定义InputSplits, 将一个文件分为不同任务
  - 为RecordReader提供一个基础，用来读取这个文件
- 格式
- 

