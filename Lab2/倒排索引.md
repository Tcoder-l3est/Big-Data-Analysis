# 可扩展的带词频倒排索引实验

## 实验原理

倒排索引（Inverted Index）被用来存储在全文搜索下某个单词在一个文档

或者一组文档中的存储位置的映射，是目前几乎所有支持全文索引的搜索引擎都

需要依赖的一个数据结构。

### 过程

- Mapper的设计

  - **Input：key=当前行偏移位置, value=当前行内容**

  - **Output：key=word#docname, value=1**

  - 读取Stop-Words
    - 使用Vector<String> 保存停词表，在setup阶段读入停词表文件。
  -  map设计
    - 通过getinputSplit 依次读取Input目录下的文件，然后把内容全部转变为小写，只保留数字、字母和空格，其余的特殊字符例如’_’ ‘.’ 等标点都不纳入计算
    - 然后去掉开头和结尾的空格方便拆分
    - 然后使用StringToken 按照空格拆分出一个个的单词，输出key = word#Filename value = 1

-  combine 的设计
  -  **Input:   key = word#filename value = 1**
  - **Output: key = word#filename, value:累加和**
  - 将Map输出的中间结果相同key部分的value累加

- Partition的设计 
  -  **Input: key = word#filename  value = sum**
  - **Do:  按word 分片，选择Reduce节点**
  -  Extends HashPartitioner，设置partition的key按照单词 而不是 单词#文件，避免不同单词会分到不同的reduce里面，导致最后统计的total不正确

- Reduce设计

  -  **Input: < "word#doc" , sum,sum...>** 
  - **Output: <word, "doc,sum 的list">**

  - 定义lastfile、lastword来记录上一个处理的文件以及单词，使用count 记录相同word和filename出现的次数，totalcount记录最后的词频

  - Reduce里面

    输入的word是排好序的

    将word,filename拆分开，将相同的filename累加和拼到一起，存在str中

    每次比较当前的word和上一次的word是否相同，若相同则将filename和累加和附加到str中

    否则输出：key:word，value:str 以及total的值

    并将新的word作为key继续

  -  使用cleanup 处理最后一个没输出的单词

## 测试

![image-20220415132707138](https://vvtorres.oss-cn-beijing.aliyuncs.com/image-20220415132707138.png)

![image-20220415132724355](https://vvtorres.oss-cn-beijing.aliyuncs.com/image-20220415132724355.png)

## 其他探索

### Partition

1. 首先在本地 运行进行测试，发现即使使用默认的partition对结果依然没有影响，猜测是datanode只有一个，查看如下

![image-20220415132801630](https://vvtorres.oss-cn-beijing.aliyuncs.com/image-20220415132801630.png)

2. 但是上传到集群的时候 有两个datanode，即使使用默认的partition，结果仍然是有序的，或者说仅仅输出了一个文件

3. 再思考，发现是partition时默认设置的reducetask 不是和datanode的数量一样，而是默认为1，所以输出一个文件或者分区分到一个datanode，自然结果不管怎么分区都一样。

4.  web查看maptask 以及 reducetask数量

   ![image-20220415132830880](https://vvtorres.oss-cn-beijing.aliyuncs.com/image-20220415132830880.png)

   默认情况下是4，1

5. 设置reducetask为2，并且使用默认的分区

   ![image-20220415132852843](https://vvtorres.oss-cn-beijing.aliyuncs.com/image-20220415132852843.png)

   可以发现输出了两个文件

   ![image-20220415132920594](https://vvtorres.oss-cn-beijing.aliyuncs.com/image-20220415132920594.png)

   但是不知为何没有出现 那种 同一word 分在不同文件的情况..

6. 于是继续调大reducetasks = 4

   发现出现了 同一个word 在不同 分区中了，即输出到不同文件，这样统计的total会失准，体现了按照key = word进行partition的必要性。

   ![image-20220415132932905](https://vvtorres.oss-cn-beijing.aliyuncs.com/image-20220415132932905.png)



## 结论

1. 查看hadoop集群有多少节点（hdfs fsck /）

2. 如果按照多个分区 最终怎么实现一个大文件的合并呢？

   需要进行全排序

   MapReduce内部包含一个TotalOrderPartitoner的分区实现类，主要解决全排序问题。其原理和自定义分区函数实现全局排序类似，根据key的分界点将不同的key发送到不同的reduce中，区别在于需要人工寻找分界点，而该方法是有程序计算得到。即[外排序与MapReduce的Sort](https://blog.csdn.net/qq_47865838/article/details/124000074?spm=1001.2014.3001.5502)

3. 为什么不论是否按word进行partition还是按照word#filename进行partition结果都没有发生改变呢？

   原因是partition默认使用NumReduceTasks=1，即一个ReduceTask，所以无论怎么样 不会出现那种按照word#filename 进行partition hash，出现相同word出现在不同reduce(输出文件)中的情况

   所以当把NumReduceTask 数目设为>=4 就会看到上述现象了

4. MapTask的数量为什么是4？

   因为MapTask 的数量由切片决定

   切片包括文件切片以及block切片，即默认情况下，每输入一个文件会自动多一个切片，本实验是4个输入文档，所以MapTask数量是4

   如果一个文件太大超过默认的块大小(128M) 则也会进行切片，进而增加MapTask的数量。

总结：明白map 以及 maptask  reduce 以及 reducetask机制 以及相关的 分区机制



## 代码

```java
package example;
import java.io.IOException;
import java.util.StringTokenizer;
import java.io.BufferedReader;
import java.util.Vector;
import java.io.InputStream;
import java.io.InputStreamReader;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.Reducer;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.input.FileSplit;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.mapreduce.lib.partition.HashPartitioner;
import org.apache.hadoop.util.GenericOptionsParser;

public class InvertIndex {
	//public static Configuration conf;
	public static class Map extends Mapper<Object, Text, Text, IntWritable> {
		/**
		 * setup():put stop words into vector<String>
		 */
        Vector<String> stop_words;     //停词表
        protected void setup(Context context) throws IOException {
        	stop_words = new Vector<String>();//初始化停词表
        	Configuration conf = context.getConfiguration();
        	// 本地读取停词表文件
        	// BufferedReader reader = new BufferedReader(new InputStreamReader(FileSystem.get(conf).open(new Path("hdfs://localhost:9000/lab2/input/stop_words_eng.txt"))));
        	BufferedReader reader = new BufferedReader(new InputStreamReader(FileSystem.get(conf).open(new Path("hdfs://10.102.0.198:9000/stop_words/stop_words_eng.txt"))));
        	String line;
        	// 按行处理
            while ((line = reader.readLine()) != null) {
            	StringTokenizer itr = new StringTokenizer(line);
            	// 遍历单词
        		while(itr.hasMoreTokens()){
        			// 新词加入停词表，默认是不重复的
        			stop_words.add(itr.nextToken());
        		}
            }
            reader.close();
        }
        
        /**
         * map():
         * 对输入的Text切分为多个word
         * Input：key=当前行偏移位置, value=当前行内容
         * Output：key=word#docname, value=1
         */
        protected void map(Object key, Text value, Context context) throws IOException, InterruptedException {
        	// 目录下文件分割
        	FileSplit fileSplit = (FileSplit) context.getInputSplit();
        	// 获取文件名
            String fileName = fileSplit.getPath().getName();
            // 将行内容全部转为小写字母
            String line = value.toString().toLowerCase();
            // 处理内容只保留数字和字母和空格
            String new_line="";
            for(int i = 0; i < line.length(); i ++) {
            	if((line.charAt(i)>='a' && line.charAt(i)<='z') || (line.charAt(i)>='0' && line.charAt(i)<='9')) {
            		new_line += line.charAt(i);
            	} else {
            		new_line +=" ";//其他字符保存为空格
            	}
            }
            // 去掉开头和结尾的空格
            line = new_line.trim();
            // 按照空格拆分
            StringTokenizer strToken = new StringTokenizer(line);
            while(strToken.hasMoreTokens()){
            	String str = strToken.nextToken();
            	// 不在停词表中 则输出key = word,docname value = 1
            	if(!stop_words.contains(str)) {
            		context.write(new Text(str+"#"+fileName), new IntWritable(1));
            	}
            }
        }
	}
 
    public static class Combine extends Reducer<Text, IntWritable, Text, IntWritable> {
        /**
         * reduce()
         * 将Map输出的中间结果相同key部分的value累加，减少向Reduce节点传输的数据量
         * Input: word#filename value = 1
         * 输出：key:word#filename, value:累加和
         */
        protected void reduce(Text key, Iterable<IntWritable> values, Context context) throws IOException, InterruptedException {
            int sum = 0;
            for (IntWritable val : values) {
                sum++;
            }
            context.write(key, new IntWritable(sum));
        }
    }
    // 分片 确保 同一word 分到一个 节点
    public static class Partition extends HashPartitioner<Text, IntWritable> {
        /**
         * getPartition()
         * Input: key = word#filename   value = sum
         * Do:    按word 分片，选择Reduce节点
         * 
         */
        public int getPartition(Text key, IntWritable value, int numReduceTasks) {
        //第三个参数numPartitions表示每个Mapper的分片数，也就是Reducer的个数
        	// 分割 提取单词
            String term = key.toString().split("#")[0];
            return super.getPartition(new Text(term), value, numReduceTasks);        
        }
    }
    public static class Reduce extends Reducer<Text, IntWritable, Text, Text> {
    	// 上一次filenmae
        private String lastfile = null;//存储上一个filename
        // 上一个 word
        private String lastword = null;//存储上一个word
        // value = str 输出的内容 key = word  value = filename sum
        private String str = "";//存储要输出的value内容
        private int count = 0;
        private int totalcount = 0;

        /**
         * Reduce()
         * Input: < "word#doc" , sum,sum...>  
         * Output: <word, "doc,sum 的list">
         * 输入的word是排好序的
         * 将word,filename拆分开，将相同的filename累加和拼到一起，存在str中
         * 每次比较当前的word和上一次的word是否相同，若相同则将filename和累加和附加到str中
         * 否则输出：key:word，value:str
         * 并将新的word作为key继续
         */
        protected void reduce(Text key, Iterable<IntWritable> values, Context context) throws IOException, InterruptedException {
        	// 将word和filename存在tokens数组中
            String[] tokens = key.toString().split("#");
            System.out.println("This is Reduce Node and key = "+ key +" values = "+values); 
            // 第一次 第一个单词的初始化
            if(lastword == null) {
            	lastword = tokens[0];
            }
            if(lastfile == null) {
            	lastfile = tokens[1];
            }
            //	此次word与上次不一样，则将lastword进行处理并输出
            if (!tokens[0].equals(lastword)) {
            	// str is value = <doc,sum> <doc,sum>;<total,totalcount>
                str += "<"+lastfile+","+count+">;<total,"+totalcount+">.";
                // 输出key value
                context.write(new Text(lastword), new Text(str));
                // 更新word,filename
                lastword = tokens[0];
                lastfile = tokens[1];
                // 重置count str
                count = 0;
                str="";
                // 累加相同word和filename中出现次数
                for (IntWritable val : values) {
                	count += val.get();//转为int
                }
                totalcount = count;
                return;
            }
        	// doc不同 更新文档名
            if(!tokens[1].equals(lastfile)) {
            	str += "<"+lastfile+","+count+">;";
            	// 更新文档名
            	lastfile = tokens[1];
            	// 重设count值
            	count = 0;
            	// 计数
            	for (IntWritable value : values){
            		count += value.get();
                }
            	totalcount += count;
            	return;
            }
            
            // 继续count
            for (IntWritable val : values) {
            	count += val.get();
            	totalcount += val.get();
            }
            
        }
 
        /**
         * 对于最后一个word额外的处理
         * 重载cleanup()，处理最后一个word并输出-yarn
         */
        public void cleanup(Context context) throws IOException, InterruptedException {
            str += "<"+lastfile+","+count+">;<total,"+totalcount+">.";
            context.write(new Text(lastword), new Text(str));
            
            super.cleanup(context);
        }
    }
	
	public static void main(String args[]) throws Exception {
		//init();
		
		//hadoop fs -get hdfs://10.102.0.198:9000/user/bigdata_201900180091/output/part-r-00000
		// hadoop fs -rm -r hdfs://10.102.0.198:9000/user/bigdata_201900180091/output/
//		System.setProperty("HADOOP_USER_NAME", "root");
		Configuration conf = new Configuration();
		// conf.set("fs.defaultFS", "hdfs://localhost:9000");
		conf.set("fs.defaultFS", "hdfs://10.102.0.198:9000");
        // String[] otherArgs=new String[]{"/hdfs://","/lab2/output"}; //直接设置输入参数 设置程序运行参数
        String[] otherArgs=new String[]{"hdfs://10.102.0.198:9000/input","hdfs://10.102.0.198:9000/user/bigdata_201900180091/output4"}; //直接设置输入参数 设置程序运行参数
        args = otherArgs;
        
        if(args.length != 2) {
            System.err.println("Usage: Relation <in> <out>");
            System.exit(2);
        }

        Job job = Job.getInstance(conf, "InvertIndex");//设置环境参数
        job.setJarByClass(InvertIndex.class);//设置整个程序的类名
        job.setMapperClass(Map.class);//设置Mapper类
        job.setCombinerClass(Combine.class);//设置combiner类
        job.setPartitionerClass(Partition.class);//设置Partitioner类
        // 设置多个Reduce 看效果
        job.setNumReduceTasks(4);
        job.setReducerClass(Reduce.class);//设置reducer类
        job.setOutputKeyClass(Text.class);//设置Mapper输出key类型
        job.setOutputValueClass(IntWritable.class);//设置Mapper输出value类型
        FileInputFormat.addInputPath(job, new Path(args[0]));//输入文件目录
        FileOutputFormat.setOutputPath(job, new Path(args[1]));//输出文件目录
        System.exit(job.waitForCompletion(true) ? 0 : 1);//参数true表示检查并打印 Job 和 Task 的运行状况
	}

}

```

